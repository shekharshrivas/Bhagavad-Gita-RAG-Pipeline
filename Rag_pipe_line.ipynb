{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shekharshrivas/bhagwat_geeta_chatbot/blob/main/Rag_pipe_line.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "TQgrdXEhS6yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHWjiP_x_ux3"
      },
      "source": [
        "**Using FAISS**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97CTNDq5U1f9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu transformers pandas numpy\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# import pandas as pd\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# csv = '/content/drive/MyDrive/Deep learning - Geeta Assignment 1/Bhagwad_Gita_Verses_English_Questions.csv'\n",
        "# df = pd.read_csv(csv)\n",
        "# df = df.drop(columns=[\"chapter\", \"verse\", \"speaker\", \"sanskrit\"])\n",
        "# faiss_df = df"
      ],
      "metadata": {
        "id": "hq63jkGuHVBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vay9t034ADQR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "faiss_df = pd.read_csv('Bhagwad_Gita_Verses_English_Questions.csv')\n",
        "\n",
        "# Combine the question and translation columns for better context in retrieval\n",
        "faiss_df['context'] = faiss_df['question'].fillna('') + \" \" + faiss_df['translation']\n",
        "\n",
        "# Display the dataset\n",
        "# print(df.head())\n",
        "# faiss_df['context']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ztfN-_kuHMlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quxkI_sTAQbo",
        "outputId": "1dc8a801-ef60-4d32-f39b-cab7ce4114a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load the BERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to get sentence embeddings\n",
        "def FAISS_get_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # Use the mean of the last hidden states to get the sentence embedding\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings.squeeze().cpu().numpy().astype('float32')  # Ensure float32\n",
        "\n",
        "# Assume faiss_df is already defined with a 'context' column\n",
        "# Compute embeddings for all content (combined question + translation)\n",
        "faiss_df['embedding'] = faiss_df['context'].apply(lambda x: FAISS_get_embedding(x))\n",
        "\n",
        "# Convert embeddings to numpy array\n",
        "embeddings = np.vstack(faiss_df['embedding'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm66nTsAA1VA",
        "outputId": "b08224b6-e1d6-4d30-b63f-38c4f3087283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of vectors in the index: 700\n"
          ]
        }
      ],
      "source": [
        "# Initialize FAISS index\n",
        "embedding_dimension = embeddings.shape[1]  # Dimension of embeddings\n",
        "index = faiss.IndexFlatL2(embedding_dimension)\n",
        "\n",
        "# Add embeddings to the index\n",
        "index.add(embeddings)\n",
        "\n",
        "print(f\"Number of vectors in the index: {index.ntotal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wknzb4hBBO7"
      },
      "outputs": [],
      "source": [
        "# Function to retrieve similar verses using FAISS\n",
        "def FAISS_retrieve_similar_verses(query, top_k=5):\n",
        "    query_embedding = FAISS_get_embedding(query).reshape(1, -1)\n",
        "\n",
        "    # Ensure query_embedding is float32\n",
        "    query_embedding = np.array(query_embedding, dtype='float32')\n",
        "\n",
        "    # Search FAISS index for the nearest neighbors\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # Get the most similar content (without the chapter and question)\n",
        "    results = faiss_df.iloc[indices[0]]['translation'].values\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXi_AqnrBHXm"
      },
      "outputs": [],
      "source": [
        "def FAISS_generate_answer(query, top_k=5):\n",
        "    # Retrieve similar verses\n",
        "    similar_verses = FAISS_retrieve_similar_verses(query, top_k=top_k)\n",
        "\n",
        "    # Combine the results into a single response (translation only)\n",
        "    answer = ' '.join(similar_verses)\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFEniy8fBM-E"
      },
      "outputs": [],
      "source": [
        "# # Example queries\n",
        "# query_1 = \"How does the Gita start?\"\n",
        "# question = \"What did Dhritarashtra say in the first chapter?\"\n",
        "# query_2 = \"What did Duryodhana say to Drona?\"\n",
        "\n",
        "# print(\"Answer 1:\", FAISS_generate_answer(query_1))\n",
        "# print(\"Answer Q:\", FAISS_generate_answer(question))\n",
        "# print(\"Answer 2:\", FAISS_generate_answer(query_2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-gyXW9FHGSr"
      },
      "source": [
        "**Dense Passage Retrieval (DPR). Retrieval algo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUvL9RQbHLlj"
      },
      "outputs": [],
      "source": [
        "!pip install transformers faiss-cpu pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltBLq7HyHbXA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Bhagwad_Gita_Verses_English_Questions.csv')\n",
        "\n",
        "# Use only the translation column for context and store the question column embeddings\n",
        "df['context'] = df['translation']  # Only using translation for the context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szSxzoRTHgm_",
        "outputId": "fd966ce0-d759-4aa8-c5f8-fabfd314b91f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Load DPR models and tokenizers for question and context\n",
        "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "\n",
        "# Load DPR model and tokenizer for the question encoder\n",
        "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "question_model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "\n",
        "# Load DPR model and tokenizer for the context (passage) encoder\n",
        "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "context_model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "# Function to get question embeddings\n",
        "def DPR_get_question_embedding(text):\n",
        "    inputs = question_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        embedding = question_model(**inputs).pooler_output\n",
        "    return embedding.cpu().numpy().astype('float32')  # Ensure float32\n",
        "\n",
        "# Function to get context (translation) embeddings\n",
        "def DPR_get_context_embedding(text):\n",
        "    inputs = context_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        embedding = context_model(**inputs).pooler_output\n",
        "    return embedding.cpu().numpy().astype('float32')  # Ensure float32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xnw3TuXqHrdZ",
        "outputId": "7bf672e0-60b2-4f19-a7b7-d1dd60ee80a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "# IT TAKES TIME, DON'T RUN IT MANY TIMES\n",
        "\n",
        "# Compute embeddings for all questions (to match queries later)\n",
        "df['question_embedding'] = df['question'].apply(lambda x: DPR_get_question_embedding(x).squeeze())\n",
        "\n",
        "# Compute embeddings for all contexts (translations)\n",
        "df['context_embedding'] = df['context'].apply(lambda x: DPR_get_context_embedding(x).squeeze())\n",
        "\n",
        "# Convert question embeddings to a numpy array\n",
        "question_embeddings = np.vstack(df['question_embedding'].values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize FAISS index for question embeddings\n",
        "embedding_dimension = question_embeddings.shape[1]  # Dimension of question embeddings\n",
        "index = faiss.IndexFlatL2(embedding_dimension)\n",
        "\n",
        "# Add question embeddings to the FAISS index\n",
        "index.add(question_embeddings)\n",
        "\n",
        "print(f\"Number of vectors in the index: {index.ntotal}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hFZkPG7ykVK",
        "outputId": "c95d540b-ec81-4bbe-809f-f84f631658af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of vectors in the index: 700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHG_lkbeH7L6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to retrieve similar verses based on query\n",
        "def DPR_retrieve_similar_verses(query, top_k=5):\n",
        "    query_embedding = DPR_get_question_embedding(query).reshape(1, -1)\n",
        "\n",
        "    # Ensure query_embedding is float32\n",
        "    query_embedding = np.array(query_embedding, dtype='float32')\n",
        "\n",
        "    # Search FAISS index for the nearest neighbors (question embeddings)\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # Get the most similar context (translation column)\n",
        "    results = df.iloc[indices[0]]['context'].values\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXetgn4HIwZg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to generate the final answer based on the retrieved context\n",
        "def DPR_generate_answer(query, top_k=5):\n",
        "    # Retrieve similar verses\n",
        "    similar_verses = DPR_retrieve_similar_verses(query, top_k=top_k)\n",
        "\n",
        "    # Combine the results into a single response (translation only)\n",
        "    answer = ' '.join(similar_verses)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8uNC6Q4I6ap"
      },
      "outputs": [],
      "source": [
        "# # Example queries\n",
        "# query_1 = \"How does the Gita start?\"\n",
        "# query_2 = \"What did Duryodhana say to Drona?\"\n",
        "\n",
        "# # Generating answers for the queries\n",
        "# print(\"Answer 1:\", DPR_generate_answer(query_1))\n",
        "# print(\"Answer 2:\", DPR_generate_answer(query_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdmEKcoaJAHy"
      },
      "outputs": [],
      "source": [
        "# # Example queries\n",
        "# query_1 = \"How does the Gita start?\"\n",
        "# question = \"What did Dhritarashtra say in the first chapter?\"\n",
        "# query_2 = \"What did Duryodhana say to Drona?\"\n",
        "\n",
        "# print(\"Answer 1:\", DPR_generate_answer(query_1))\n",
        "# print(\"Answer Q:\", DPR_generate_answer(question))\n",
        "# print(\"Answer 2:\", DPR_generate_answer(query_2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**_______________BM25______________**"
      ],
      "metadata": {
        "id": "Fkx3I3J3OFh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install rank_bm25\n",
        "!pip3 install faiss-cpu transformers pandas numpy\n",
        "!pip3 install sentence-transformers\n",
        "!pip3 install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "tK188KglOrTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from rank_bm25 import BM25Okapi\n",
        "import pandas as pd\n",
        "\n",
        "# Download the required NLTK dataset\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVhurSxqOEEd",
        "outputId": "2d47e72b-ccfa-4525-93bf-55ddab915f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Bhagavad Gita data\n",
        "data = pd.read_csv(\"Bhagwad_Gita_Verses_English_Questions.csv\")\n",
        "verses = data['translation'].tolist()"
      ],
      "metadata": {
        "id": "Fumvli__OD8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the verses\n",
        "tokenized_verses = [word_tokenize(verse.lower()) for verse in verses]\n",
        "\n",
        "# Initialize BM25 model\n",
        "bm25 = BM25Okapi(tokenized_verses)"
      ],
      "metadata": {
        "id": "YlCWqGeVOD1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve context\n",
        "def bm25_retrieve_context(query, n=25):\n",
        "    tokenized_query = word_tokenize(query.lower())\n",
        "    scores = bm25.get_scores(tokenized_query)\n",
        "    top_n_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:n]\n",
        "    return [verses[i] for i in top_n_indices]\n"
      ],
      "metadata": {
        "id": "qM7XOTSdODrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example query\n",
        "# query = \"What is the nature of the soul?\"\n",
        "# context = bm25_retrieve_context(query)\n",
        "# print(\"Retrieved Context:\", context)"
      ],
      "metadata": {
        "id": "HmdpCHddODfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "INZs-8SoPGJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4LCBCISPGFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qm5-6p4cPGC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jYX4huxWPF_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W6J5F_4RPFg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scW6E8VxQ3tN"
      },
      "source": [
        "**Generation using Grop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cAkFmjUJJ9k"
      },
      "outputs": [],
      "source": [
        "# Gemini_API_Key = \"AIzaSyARy_-sa09dpRrj2jEVWXE_sJ-0sXZo_QY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMYslT1qbPp7"
      },
      "outputs": [],
      "source": [
        "# from groq import Groq\n",
        "# groqApi = \"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\"\n",
        "# groqApi = \"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\"\n",
        "# groqApi = \"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\"\n",
        "# groqApi = \"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\"\n",
        "groqApi = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwJ_pEIhiOW0"
      },
      "outputs": [],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFpTlT8ah2JU"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "74pkiatzfgMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KBcluMVfxNc"
      },
      "outputs": [],
      "source": [
        "####### groqAPI ######\n",
        "def llmResponse(query, context):\n",
        "    client = Groq(\n",
        "        api_key=groqApi,\n",
        "    )\n",
        "\n",
        "    # Construct the message content using f-strings for proper formatting\n",
        "    message_content = f\"\"\"Answer the question based only on the following context:\n",
        "    {context}\n",
        "    Question: {query}\n",
        "    \"\"\"\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": message_content,\n",
        "            }\n",
        "        ],\n",
        "        model=\"llama3-8b-8192\",\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JIreOJliWj9"
      },
      "outputs": [],
      "source": [
        "query = \"How does the Gita start?\"\n",
        "context = DPR_generate_answer(query)\n",
        "ans = llmResponse(query, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjvyak_KiiRk",
        "outputId": "fc3fefdf-e35d-4c77-8c0a-2f1a965680bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer: The Gita starts with Dhritarashtra asking Sanjaya what happened when his people and the sons of Pandu assembled together on the holy plain of Kurukshetra, eager for battle.\n"
          ]
        }
      ],
      "source": [
        "print(\"Generated Answer:\", ans)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################## \"GeetaQA - Sheet1.csv\" #####################################################################"
      ],
      "metadata": {
        "id": "NX8jwl9uo7BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch"
      ],
      "metadata": {
        "id": "pMejr4R6o69k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained tokenizer and model for embedding generation\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obPAS8VKo67A",
        "outputId": "79d99947-e757-4a51-d383-4163ba132f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get embeddings\n",
        "def get_embeddings(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    outputs = model(**inputs)\n",
        "    # Use the mean pooling for sentence embeddings\n",
        "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "QJE8_MGpo64B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate cosine similarity\n",
        "def calculate_cosine_similarity(gen_answer, actual_answer):\n",
        "    gen_embedding = get_embeddings(gen_answer).detach().numpy()\n",
        "    actual_embedding = get_embeddings(actual_answer).detach().numpy()\n",
        "    return cosine_similarity(gen_embedding, actual_embedding)[0][0]\n"
      ],
      "metadata": {
        "id": "mx9cojvRo60z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "def evaluate_answer(query, context, actual_answer):\n",
        "    generated_answer = llmResponse(query, context)\n",
        "    similarity = calculate_cosine_similarity(generated_answer, actual_answer)\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "6pk9q7Hqo6yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your data\n",
        "data = pd.read_csv(\"GeetaQA - Sheet1 (1).csv\")"
      ],
      "metadata": {
        "id": "f4yxgjWao6wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query = data[\"Question\"]\n",
        "actual_answer = data[\"Answer\"]\n",
        "context = \"your retrieved context here\"  # Replace with the actual context from BM25 retrieval\n",
        "\n",
        "# Calculate similarity\n",
        "# similarity_score = evaluate_answer(query, context, actual_answer)\n",
        "# print(\"Cosine Similarity:\", similarity_score)"
      ],
      "metadata": {
        "id": "ADsAkK8Jo6s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G9uSJ4Upo6qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faissLst = []\n",
        "\n",
        "for i in range(len(query)):\n",
        "  q = query[i]\n",
        "  ans = actual_answer[i]\n",
        "  # print(ans)\n",
        "  context = FAISS_generate_answer(str(q), top_k=5)\n",
        "  # print(type(ans))\n",
        "  # print()\n",
        "  similarity_score = evaluate_answer(str(q), str(context), str(actual_answer))\n",
        "  print(f\"Cosine Similarity of Q{i} using FAISS:\", similarity_score)\n",
        "  faissLst.append(similarity_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97eN0gGAv00n",
        "outputId": "28e22409-7c09-4812-d12f-f2f48d6ac241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity of Q0 using FAISS: 0.49818212\n",
            "Cosine Similarity of Q1 using FAISS: 0.39594764\n",
            "Cosine Similarity of Q2 using FAISS: 0.32634908\n",
            "Cosine Similarity of Q3 using FAISS: 0.39081442\n",
            "Cosine Similarity of Q4 using FAISS: 0.4245929\n",
            "Cosine Similarity of Q5 using FAISS: 0.36452606\n",
            "Cosine Similarity of Q6 using FAISS: 0.40719128\n",
            "Cosine Similarity of Q7 using FAISS: 0.40369838\n",
            "Cosine Similarity of Q8 using FAISS: 0.3958642\n",
            "Cosine Similarity of Q9 using FAISS: 0.3136869\n",
            "Cosine Similarity of Q10 using FAISS: 0.23539957\n",
            "Cosine Similarity of Q11 using FAISS: 0.30187446\n",
            "Cosine Similarity of Q12 using FAISS: 0.18960789\n",
            "Cosine Similarity of Q13 using FAISS: 0.40392125\n",
            "Cosine Similarity of Q14 using FAISS: 0.31200337\n",
            "Cosine Similarity of Q15 using FAISS: 0.2909149\n",
            "Cosine Similarity of Q16 using FAISS: 0.4184265\n",
            "Cosine Similarity of Q17 using FAISS: 0.35139003\n",
            "Cosine Similarity of Q18 using FAISS: 0.2240963\n",
            "Cosine Similarity of Q19 using FAISS: 0.34903127\n",
            "Cosine Similarity of Q20 using FAISS: 0.41933525\n",
            "Cosine Similarity of Q21 using FAISS: 0.43096015\n",
            "Cosine Similarity of Q22 using FAISS: 0.44076774\n",
            "Cosine Similarity of Q23 using FAISS: 0.3659429\n",
            "Cosine Similarity of Q24 using FAISS: 0.36155164\n",
            "Cosine Similarity of Q25 using FAISS: 0.3452428\n",
            "Cosine Similarity of Q26 using FAISS: 0.37000543\n",
            "Cosine Similarity of Q27 using FAISS: 0.4572534\n",
            "Cosine Similarity of Q28 using FAISS: 0.37397566\n",
            "Cosine Similarity of Q29 using FAISS: 0.38142788\n",
            "Cosine Similarity of Q30 using FAISS: 0.36162254\n",
            "Cosine Similarity of Q31 using FAISS: 0.3683849\n",
            "Cosine Similarity of Q32 using FAISS: 0.39662486\n",
            "Cosine Similarity of Q33 using FAISS: 0.29607502\n",
            "Cosine Similarity of Q34 using FAISS: 0.33968973\n",
            "Cosine Similarity of Q35 using FAISS: 0.21580946\n",
            "Cosine Similarity of Q36 using FAISS: 0.40592146\n",
            "Cosine Similarity of Q37 using FAISS: 0.4418153\n",
            "Cosine Similarity of Q38 using FAISS: 0.42232502\n",
            "Cosine Similarity of Q39 using FAISS: 0.29011315\n",
            "Cosine Similarity of Q40 using FAISS: 0.36673588\n",
            "Cosine Similarity of Q41 using FAISS: 0.3697783\n",
            "Cosine Similarity of Q42 using FAISS: 0.29872394\n",
            "Cosine Similarity of Q43 using FAISS: 0.4511702\n",
            "Cosine Similarity of Q44 using FAISS: 0.38314956\n",
            "Cosine Similarity of Q45 using FAISS: 0.44342732\n",
            "Cosine Similarity of Q46 using FAISS: 0.39536554\n",
            "Cosine Similarity of Q47 using FAISS: 0.36840373\n",
            "Cosine Similarity of Q48 using FAISS: 0.51785415\n",
            "Cosine Similarity of Q49 using FAISS: 0.41703016\n",
            "Cosine Similarity of Q50 using FAISS: 0.43969482\n",
            "Cosine Similarity of Q51 using FAISS: 0.39550292\n",
            "Cosine Similarity of Q52 using FAISS: 0.29999572\n",
            "Cosine Similarity of Q53 using FAISS: 0.2691062\n",
            "Cosine Similarity of Q54 using FAISS: 0.4292655\n",
            "Cosine Similarity of Q55 using FAISS: 0.27894443\n",
            "Cosine Similarity of Q56 using FAISS: 0.33482313\n",
            "Cosine Similarity of Q57 using FAISS: 0.38720545\n",
            "Cosine Similarity of Q58 using FAISS: 0.4575717\n",
            "Cosine Similarity of Q59 using FAISS: 0.48779228\n",
            "Cosine Similarity of Q60 using FAISS: 0.2510338\n",
            "Cosine Similarity of Q61 using FAISS: 0.2054348\n",
            "Cosine Similarity of Q62 using FAISS: 0.20445468\n",
            "Cosine Similarity of Q63 using FAISS: 0.1510074\n",
            "Cosine Similarity of Q64 using FAISS: 0.2870127\n",
            "Cosine Similarity of Q65 using FAISS: 0.212273\n",
            "Cosine Similarity of Q66 using FAISS: 0.308545\n",
            "Cosine Similarity of Q67 using FAISS: 0.28341073\n",
            "Cosine Similarity of Q68 using FAISS: 0.19209726\n",
            "Cosine Similarity of Q69 using FAISS: 0.23598287\n",
            "Cosine Similarity of Q70 using FAISS: 0.08689378\n",
            "Cosine Similarity of Q71 using FAISS: 0.38252723\n",
            "Cosine Similarity of Q72 using FAISS: 0.2898984\n",
            "Cosine Similarity of Q73 using FAISS: 0.36329067\n",
            "Cosine Similarity of Q74 using FAISS: 0.11396688\n",
            "Cosine Similarity of Q75 using FAISS: 0.32292518\n",
            "Cosine Similarity of Q76 using FAISS: 0.11295523\n",
            "Cosine Similarity of Q77 using FAISS: -0.009641826\n",
            "Cosine Similarity of Q78 using FAISS: 0.27088612\n",
            "Cosine Similarity of Q79 using FAISS: 0.22064859\n",
            "Cosine Similarity of Q80 using FAISS: 0.2952759\n",
            "Cosine Similarity of Q81 using FAISS: 0.20778877\n",
            "Cosine Similarity of Q82 using FAISS: 0.3023213\n",
            "Cosine Similarity of Q83 using FAISS: 0.31678385\n",
            "Cosine Similarity of Q84 using FAISS: 0.15305814\n",
            "Cosine Similarity of Q85 using FAISS: 0.32552606\n",
            "Cosine Similarity of Q86 using FAISS: 0.27342546\n",
            "Cosine Similarity of Q87 using FAISS: 0.31240523\n",
            "Cosine Similarity of Q88 using FAISS: 0.35641634\n",
            "Cosine Similarity of Q89 using FAISS: 0.42123184\n",
            "Cosine Similarity of Q90 using FAISS: 0.19857684\n",
            "Cosine Similarity of Q91 using FAISS: 0.46890032\n",
            "Cosine Similarity of Q92 using FAISS: 0.3458754\n",
            "Cosine Similarity of Q93 using FAISS: 0.36300993\n",
            "Cosine Similarity of Q94 using FAISS: 0.3694781\n",
            "Cosine Similarity of Q95 using FAISS: 0.43640304\n",
            "Cosine Similarity of Q96 using FAISS: 0.35432425\n",
            "Cosine Similarity of Q97 using FAISS: 0.32945514\n",
            "Cosine Similarity of Q98 using FAISS: 0.357621\n",
            "Cosine Similarity of Q99 using FAISS: 0.400416\n",
            "Cosine Similarity of Q100 using FAISS: 0.29133677\n",
            "Cosine Similarity of Q101 using FAISS: 0.4193569\n",
            "Cosine Similarity of Q102 using FAISS: 0.11868625\n",
            "Cosine Similarity of Q103 using FAISS: 0.36950114\n",
            "Cosine Similarity of Q104 using FAISS: 0.1571123\n",
            "Cosine Similarity of Q105 using FAISS: 0.44851512\n",
            "Cosine Similarity of Q106 using FAISS: 0.2727496\n",
            "Cosine Similarity of Q107 using FAISS: 0.31145978\n",
            "Cosine Similarity of Q108 using FAISS: 0.27201897\n",
            "Cosine Similarity of Q109 using FAISS: 0.35373223\n",
            "Cosine Similarity of Q110 using FAISS: 0.3209656\n",
            "Cosine Similarity of Q111 using FAISS: 0.24627951\n",
            "Cosine Similarity of Q112 using FAISS: 0.37769127\n",
            "Cosine Similarity of Q113 using FAISS: 0.3800251\n",
            "Cosine Similarity of Q114 using FAISS: 0.4302531\n",
            "Cosine Similarity of Q115 using FAISS: 0.19861995\n",
            "Cosine Similarity of Q116 using FAISS: 0.39601895\n",
            "Cosine Similarity of Q117 using FAISS: 0.3948537\n",
            "Cosine Similarity of Q118 using FAISS: 0.51945597\n",
            "Cosine Similarity of Q119 using FAISS: 0.42448568\n",
            "Cosine Similarity of Q120 using FAISS: 0.46426594\n",
            "Cosine Similarity of Q121 using FAISS: 0.2904421\n",
            "Cosine Similarity of Q122 using FAISS: 0.1687356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Avg of the FAISS Accuracy:\",sum(faissLst)/len(faissLst))"
      ],
      "metadata": {
        "id": "2WZAxCWgv0x_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2334c23c-cd87-4e5e-a33a-7281af50f57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg of the FAISS Accuracy: 0.33312488371521476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avg of the FAISS Accuracy: 0.4727143384516239"
      ],
      "metadata": {
        "id": "-YpKNnrTOW2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DPRLst = []\n",
        "\n",
        "for i in range(len(query)):\n",
        "  q = query[i]\n",
        "  ans = actual_answer[i]\n",
        "  context = DPR_generate_answer(q, top_k=5)\n",
        "  # print(type(ans))\n",
        "  # print()\n",
        "  similarity_score = evaluate_answer(str(q), str(context), str(actual_answer))\n",
        "  print(f\"Cosine Similarity of Q{i} using DPR:\", similarity_score)\n",
        "  DPRLst.append(similarity_score)\n"
      ],
      "metadata": {
        "id": "Ofsc9278Cc1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d732337f-eefb-40a4-aba3-57ddd9e977ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity of Q0 using DPR: 0.4892843\n",
            "Cosine Similarity of Q1 using DPR: 0.3759182\n",
            "Cosine Similarity of Q2 using DPR: 0.27807727\n",
            "Cosine Similarity of Q3 using DPR: 0.38511494\n",
            "Cosine Similarity of Q4 using DPR: 0.45975423\n",
            "Cosine Similarity of Q5 using DPR: 0.4103304\n",
            "Cosine Similarity of Q6 using DPR: 0.40260684\n",
            "Cosine Similarity of Q7 using DPR: 0.40369838\n",
            "Cosine Similarity of Q8 using DPR: 0.42600116\n",
            "Cosine Similarity of Q9 using DPR: 0.3136869\n",
            "Cosine Similarity of Q10 using DPR: 0.23695594\n",
            "Cosine Similarity of Q11 using DPR: 0.2932133\n",
            "Cosine Similarity of Q12 using DPR: 0.25014642\n",
            "Cosine Similarity of Q13 using DPR: 0.37024054\n",
            "Cosine Similarity of Q14 using DPR: 0.3287648\n",
            "Cosine Similarity of Q15 using DPR: 0.2909149\n",
            "Cosine Similarity of Q16 using DPR: 0.4370916\n",
            "Cosine Similarity of Q17 using DPR: 0.3373484\n",
            "Cosine Similarity of Q18 using DPR: 0.2093983\n",
            "Cosine Similarity of Q19 using DPR: 0.33655387\n",
            "Cosine Similarity of Q20 using DPR: 0.41386762\n",
            "Cosine Similarity of Q21 using DPR: 0.42755985\n",
            "Cosine Similarity of Q22 using DPR: 0.4154389\n",
            "Cosine Similarity of Q23 using DPR: 0.39791495\n",
            "Cosine Similarity of Q24 using DPR: 0.3647739\n",
            "Cosine Similarity of Q25 using DPR: 0.33025151\n",
            "Cosine Similarity of Q26 using DPR: 0.4013474\n",
            "Cosine Similarity of Q27 using DPR: 0.41439342\n",
            "Cosine Similarity of Q28 using DPR: 0.34159976\n",
            "Cosine Similarity of Q29 using DPR: 0.3150487\n",
            "Cosine Similarity of Q30 using DPR: 0.27966145\n",
            "Cosine Similarity of Q31 using DPR: 0.3721504\n",
            "Cosine Similarity of Q32 using DPR: 0.42057037\n",
            "Cosine Similarity of Q33 using DPR: 0.27132273\n",
            "Cosine Similarity of Q34 using DPR: 0.3791836\n",
            "Cosine Similarity of Q35 using DPR: 0.2517621\n",
            "Cosine Similarity of Q36 using DPR: 0.44544563\n",
            "Cosine Similarity of Q37 using DPR: 0.4116526\n",
            "Cosine Similarity of Q38 using DPR: 0.35928434\n",
            "Cosine Similarity of Q39 using DPR: 0.29297668\n",
            "Cosine Similarity of Q40 using DPR: 0.37113148\n",
            "Cosine Similarity of Q41 using DPR: 0.4452714\n",
            "Cosine Similarity of Q42 using DPR: 0.3770002\n",
            "Cosine Similarity of Q43 using DPR: 0.42588988\n",
            "Cosine Similarity of Q44 using DPR: 0.4331857\n",
            "Cosine Similarity of Q45 using DPR: 0.38792187\n",
            "Cosine Similarity of Q46 using DPR: 0.4053668\n",
            "Cosine Similarity of Q47 using DPR: 0.36941338\n",
            "Cosine Similarity of Q48 using DPR: 0.5154242\n",
            "Cosine Similarity of Q49 using DPR: 0.4318604\n",
            "Cosine Similarity of Q50 using DPR: 0.391845\n",
            "Cosine Similarity of Q51 using DPR: 0.40235066\n",
            "Cosine Similarity of Q52 using DPR: 0.2570771\n",
            "Cosine Similarity of Q53 using DPR: 0.2553985\n",
            "Cosine Similarity of Q54 using DPR: 0.40212482\n",
            "Cosine Similarity of Q55 using DPR: 0.3351819\n",
            "Cosine Similarity of Q56 using DPR: 0.4324345\n",
            "Cosine Similarity of Q57 using DPR: 0.4220971\n",
            "Cosine Similarity of Q58 using DPR: 0.44250664\n",
            "Cosine Similarity of Q59 using DPR: 0.42203447\n",
            "Cosine Similarity of Q60 using DPR: 0.22911379\n",
            "Cosine Similarity of Q61 using DPR: 0.16214126\n",
            "Cosine Similarity of Q62 using DPR: 0.10566534\n",
            "Cosine Similarity of Q63 using DPR: 0.13489975\n",
            "Cosine Similarity of Q64 using DPR: 0.064368606\n",
            "Cosine Similarity of Q65 using DPR: 0.035117663\n",
            "Cosine Similarity of Q66 using DPR: 0.3597895\n",
            "Cosine Similarity of Q67 using DPR: 0.30516872\n",
            "Cosine Similarity of Q68 using DPR: 0.26687947\n",
            "Cosine Similarity of Q69 using DPR: 0.1437876\n",
            "Cosine Similarity of Q70 using DPR: 0.11023573\n",
            "Cosine Similarity of Q71 using DPR: 0.3105709\n",
            "Cosine Similarity of Q72 using DPR: 0.28199354\n",
            "Cosine Similarity of Q73 using DPR: 0.30978215\n",
            "Cosine Similarity of Q74 using DPR: 0.11464144\n",
            "Cosine Similarity of Q75 using DPR: 0.048585597\n",
            "Cosine Similarity of Q76 using DPR: 0.08783722\n",
            "Cosine Similarity of Q77 using DPR: 0.07390658\n",
            "Cosine Similarity of Q78 using DPR: 0.038578954\n",
            "Cosine Similarity of Q79 using DPR: 0.23520887\n",
            "Cosine Similarity of Q80 using DPR: 0.26557392\n",
            "Cosine Similarity of Q81 using DPR: 0.20778877\n",
            "Cosine Similarity of Q82 using DPR: 0.16391546\n",
            "Cosine Similarity of Q83 using DPR: 0.27115777\n",
            "Cosine Similarity of Q84 using DPR: 0.3400363\n",
            "Cosine Similarity of Q85 using DPR: 0.38779074\n",
            "Cosine Similarity of Q86 using DPR: 0.1502784\n",
            "Cosine Similarity of Q87 using DPR: 0.313739\n",
            "Cosine Similarity of Q88 using DPR: 0.40610015\n",
            "Cosine Similarity of Q89 using DPR: 0.36874872\n",
            "Cosine Similarity of Q90 using DPR: 0.2963452\n",
            "Cosine Similarity of Q91 using DPR: 0.45773944\n",
            "Cosine Similarity of Q92 using DPR: 0.30459484\n",
            "Cosine Similarity of Q93 using DPR: 0.28046083\n",
            "Cosine Similarity of Q94 using DPR: 0.37335545\n",
            "Cosine Similarity of Q95 using DPR: 0.51193136\n",
            "Cosine Similarity of Q96 using DPR: 0.3145357\n",
            "Cosine Similarity of Q97 using DPR: 0.36080873\n",
            "Cosine Similarity of Q98 using DPR: 0.32332778\n",
            "Cosine Similarity of Q99 using DPR: 0.31020573\n",
            "Cosine Similarity of Q100 using DPR: 0.19104931\n",
            "Cosine Similarity of Q101 using DPR: 0.42239058\n",
            "Cosine Similarity of Q102 using DPR: 0.12485191\n",
            "Cosine Similarity of Q103 using DPR: 0.3552943\n",
            "Cosine Similarity of Q104 using DPR: 0.16551283\n",
            "Cosine Similarity of Q105 using DPR: 0.2610549\n",
            "Cosine Similarity of Q106 using DPR: 0.31350523\n",
            "Cosine Similarity of Q107 using DPR: 0.23619169\n",
            "Cosine Similarity of Q108 using DPR: 0.22881241\n",
            "Cosine Similarity of Q109 using DPR: 0.3579998\n",
            "Cosine Similarity of Q110 using DPR: 0.28451595\n",
            "Cosine Similarity of Q111 using DPR: 0.4005443\n",
            "Cosine Similarity of Q112 using DPR: 0.37320432\n",
            "Cosine Similarity of Q113 using DPR: 0.3998856\n",
            "Cosine Similarity of Q114 using DPR: 0.39100417\n",
            "Cosine Similarity of Q115 using DPR: 0.15080665\n",
            "Cosine Similarity of Q116 using DPR: 0.3545572\n",
            "Cosine Similarity of Q117 using DPR: 0.43816072\n",
            "Cosine Similarity of Q118 using DPR: 0.4852481\n",
            "Cosine Similarity of Q119 using DPR: 0.41690463\n",
            "Cosine Similarity of Q120 using DPR: 0.3722148\n",
            "Cosine Similarity of Q121 using DPR: 0.28028843\n",
            "Cosine Similarity of Q122 using DPR: 0.33266872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Avg of the DPR Accuracy:\",sum(DPRLst)/len(DPRLst))"
      ],
      "metadata": {
        "id": "IqDKmgVQDFVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775564aa-e04b-4dfe-ff16-a03f1385ea4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg of the DPR Accuracy: 0.32057069950714345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avg of the DPR Accuracy: 0.5107990130782127"
      ],
      "metadata": {
        "id": "vCyKtEFNPNlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DPRLst = []\n",
        "\n",
        "for i in range(len(query)):\n",
        "  q = query[i]\n",
        "  ans = actual_answer[i]\n",
        "  context = DPR_generate_answer(q, top_k=5)\n",
        "  # print(type(ans))\n",
        "  # print()\n",
        "  similarity_score = evaluate_answer(str(q), str(context), str(actual_answer))\n",
        "  print(f\"Cosine Similarity of Q{i} using DPR:\", similarity_score)\n",
        "  DPRLst.append(similarity_score)\n"
      ],
      "metadata": {
        "id": "3ly3HWeZv0ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c42e7a-b5a6-4ed4-f2c9-af95a9841252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity of Q0 using DPR: 0.5407568\n",
            "Cosine Similarity of Q1 using DPR: 0.6304922\n",
            "Cosine Similarity of Q2 using DPR: 0.551347\n",
            "Cosine Similarity of Q3 using DPR: 0.38812214\n",
            "Cosine Similarity of Q4 using DPR: 0.54868263\n",
            "Cosine Similarity of Q5 using DPR: 0.42847157\n",
            "Cosine Similarity of Q6 using DPR: 0.52171445\n",
            "Cosine Similarity of Q7 using DPR: 0.62314487\n",
            "Cosine Similarity of Q8 using DPR: 0.416407\n",
            "Cosine Similarity of Q9 using DPR: 0.43930665\n",
            "Cosine Similarity of Q10 using DPR: 0.55515003\n",
            "Cosine Similarity of Q11 using DPR: 0.49054998\n",
            "Cosine Similarity of Q12 using DPR: 0.45045346\n",
            "Cosine Similarity of Q13 using DPR: 0.4652986\n",
            "Cosine Similarity of Q14 using DPR: 0.46570924\n",
            "Cosine Similarity of Q15 using DPR: 0.5689998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Avg of the DPR Accuracy:\",sum(DPRLst)/len(DPRLst))"
      ],
      "metadata": {
        "id": "SObKupcZv0rL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "104d6466-03f7-4340-8712-de8c8357c8c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg of the DPR Accuracy: 0.505287904292345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BM25Lst = []\n",
        "\n",
        "for i in range(len(query)):\n",
        "  q = query[i]\n",
        "  ans = actual_answer[i]\n",
        "  context = bm25_retrieve_context(q)\n",
        "  context = ' '.join(context)\n",
        "  # print(len(context))\n",
        "  # print()\n",
        "  similarity_score = evaluate_answer(str(q), str(context), str(actual_answer))\n",
        "  print(f\"Cosine Similarity of Q{i} using BM25:\", similarity_score)\n",
        "  BM25Lst.append(similarity_score)\n"
      ],
      "metadata": {
        "id": "3NtsvBoGDMxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1943547-5b7e-493f-ce76-0cfeba84ab63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity of Q0 using BM25: 0.4701975\n",
            "Cosine Similarity of Q1 using BM25: 0.34197116\n",
            "Cosine Similarity of Q2 using BM25: 0.25052273\n",
            "Cosine Similarity of Q3 using BM25: 0.36031896\n",
            "Cosine Similarity of Q4 using BM25: 0.48490077\n",
            "Cosine Similarity of Q5 using BM25: 0.36350602\n",
            "Cosine Similarity of Q6 using BM25: 0.39332214\n",
            "Cosine Similarity of Q7 using BM25: 0.35478216\n",
            "Cosine Similarity of Q8 using BM25: 0.3502525\n",
            "Cosine Similarity of Q9 using BM25: 0.32858974\n",
            "Cosine Similarity of Q10 using BM25: 0.20397182\n",
            "Cosine Similarity of Q11 using BM25: 0.3063754\n",
            "Cosine Similarity of Q12 using BM25: 0.271556\n",
            "Cosine Similarity of Q13 using BM25: 0.32541585\n",
            "Cosine Similarity of Q14 using BM25: 0.27486235\n",
            "Cosine Similarity of Q15 using BM25: 0.28480586\n",
            "Cosine Similarity of Q16 using BM25: 0.39388123\n",
            "Cosine Similarity of Q17 using BM25: 0.16526176\n",
            "Cosine Similarity of Q18 using BM25: 0.18001907\n",
            "Cosine Similarity of Q19 using BM25: 0.3760178\n",
            "Cosine Similarity of Q20 using BM25: 0.3663373\n",
            "Cosine Similarity of Q21 using BM25: 0.4263971\n",
            "Cosine Similarity of Q22 using BM25: 0.4533254\n",
            "Cosine Similarity of Q23 using BM25: 0.34658343\n",
            "Cosine Similarity of Q24 using BM25: 0.3647739\n",
            "Cosine Similarity of Q25 using BM25: 0.33397964\n",
            "Cosine Similarity of Q26 using BM25: 0.41057643\n",
            "Cosine Similarity of Q27 using BM25: 0.42998546\n",
            "Cosine Similarity of Q28 using BM25: 0.30636728\n",
            "Cosine Similarity of Q29 using BM25: 0.3267544\n",
            "Cosine Similarity of Q30 using BM25: 0.28509238\n",
            "Cosine Similarity of Q31 using BM25: 0.37585044\n",
            "Cosine Similarity of Q32 using BM25: 0.35825196\n",
            "Cosine Similarity of Q33 using BM25: 0.45764118\n",
            "Cosine Similarity of Q34 using BM25: 0.35921046\n",
            "Cosine Similarity of Q35 using BM25: 0.3201233\n",
            "Cosine Similarity of Q36 using BM25: 0.4654594\n",
            "Cosine Similarity of Q37 using BM25: 0.50994754\n",
            "Cosine Similarity of Q38 using BM25: 0.38128132\n",
            "Cosine Similarity of Q39 using BM25: 0.19223404\n",
            "Cosine Similarity of Q40 using BM25: 0.5148194\n",
            "Cosine Similarity of Q41 using BM25: 0.40637347\n",
            "Cosine Similarity of Q42 using BM25: 0.40485746\n",
            "Cosine Similarity of Q43 using BM25: 0.39219466\n",
            "Cosine Similarity of Q44 using BM25: 0.3326704\n",
            "Cosine Similarity of Q45 using BM25: 0.45942998\n",
            "Cosine Similarity of Q46 using BM25: 0.44327116\n",
            "Cosine Similarity of Q47 using BM25: 0.46450365\n",
            "Cosine Similarity of Q48 using BM25: 0.5020814\n",
            "Cosine Similarity of Q49 using BM25: 0.41714215\n",
            "Cosine Similarity of Q50 using BM25: 0.46256363\n",
            "Cosine Similarity of Q51 using BM25: 0.3680579\n",
            "Cosine Similarity of Q52 using BM25: 0.42537493\n",
            "Cosine Similarity of Q53 using BM25: 0.34881556\n",
            "Cosine Similarity of Q54 using BM25: 0.510086\n",
            "Cosine Similarity of Q55 using BM25: 0.39589438\n",
            "Cosine Similarity of Q56 using BM25: 0.4285473\n",
            "Cosine Similarity of Q57 using BM25: 0.42527327\n",
            "Cosine Similarity of Q58 using BM25: 0.47075075\n",
            "Cosine Similarity of Q59 using BM25: 0.46716136\n",
            "Cosine Similarity of Q60 using BM25: 0.15944749\n",
            "Cosine Similarity of Q61 using BM25: 0.4766323\n",
            "Cosine Similarity of Q62 using BM25: 0.42416003\n",
            "Cosine Similarity of Q63 using BM25: 0.3183381\n",
            "Cosine Similarity of Q64 using BM25: 0.38382515\n",
            "Cosine Similarity of Q65 using BM25: 0.36905333\n",
            "Cosine Similarity of Q66 using BM25: 0.33132356\n",
            "Cosine Similarity of Q67 using BM25: 0.29414904\n",
            "Cosine Similarity of Q68 using BM25: 0.36647645\n",
            "Cosine Similarity of Q69 using BM25: 0.32604164\n",
            "Cosine Similarity of Q70 using BM25: 0.30145514\n",
            "Cosine Similarity of Q71 using BM25: 0.26774967\n",
            "Cosine Similarity of Q72 using BM25: 0.343628\n",
            "Cosine Similarity of Q73 using BM25: 0.2754843\n",
            "Cosine Similarity of Q74 using BM25: 0.10694295\n",
            "Cosine Similarity of Q75 using BM25: 0.27492666\n",
            "Cosine Similarity of Q76 using BM25: 0.2027447\n",
            "Cosine Similarity of Q77 using BM25: 0.02370352\n",
            "Cosine Similarity of Q78 using BM25: 0.27165535\n",
            "Cosine Similarity of Q79 using BM25: 0.3142576\n",
            "Cosine Similarity of Q80 using BM25: 0.36882758\n",
            "Cosine Similarity of Q81 using BM25: 0.20471638\n",
            "Cosine Similarity of Q82 using BM25: 0.30077735\n",
            "Cosine Similarity of Q83 using BM25: 0.296953\n",
            "Cosine Similarity of Q84 using BM25: 0.32922465\n",
            "Cosine Similarity of Q85 using BM25: 0.42941254\n",
            "Cosine Similarity of Q86 using BM25: 0.22661436\n",
            "Cosine Similarity of Q87 using BM25: 0.29360223\n",
            "Cosine Similarity of Q88 using BM25: 0.4513173\n",
            "Cosine Similarity of Q89 using BM25: 0.37449384\n",
            "Cosine Similarity of Q90 using BM25: 0.32813874\n",
            "Cosine Similarity of Q91 using BM25: 0.4451683\n",
            "Cosine Similarity of Q92 using BM25: 0.3263299\n",
            "Cosine Similarity of Q93 using BM25: 0.27646327\n",
            "Cosine Similarity of Q94 using BM25: 0.47616848\n",
            "Cosine Similarity of Q95 using BM25: 0.46617502\n",
            "Cosine Similarity of Q96 using BM25: 0.32949585\n",
            "Cosine Similarity of Q97 using BM25: 0.3757754\n",
            "Cosine Similarity of Q98 using BM25: 0.3818701\n",
            "Cosine Similarity of Q99 using BM25: 0.28506547\n",
            "Cosine Similarity of Q100 using BM25: 0.21389845\n",
            "Cosine Similarity of Q101 using BM25: 0.41839346\n",
            "Cosine Similarity of Q102 using BM25: 0.13335533\n",
            "Cosine Similarity of Q103 using BM25: 0.23631983\n",
            "Cosine Similarity of Q104 using BM25: 0.2806278\n",
            "Cosine Similarity of Q105 using BM25: 0.3826007\n",
            "Cosine Similarity of Q106 using BM25: 0.26313713\n",
            "Cosine Similarity of Q107 using BM25: 0.42963162\n",
            "Cosine Similarity of Q108 using BM25: 0.38185376\n",
            "Cosine Similarity of Q109 using BM25: 0.40478265\n",
            "Cosine Similarity of Q110 using BM25: 0.13932726\n",
            "Cosine Similarity of Q111 using BM25: 0.40298504\n",
            "Cosine Similarity of Q112 using BM25: 0.4883441\n",
            "Cosine Similarity of Q113 using BM25: 0.33955258\n",
            "Cosine Similarity of Q114 using BM25: 0.47101712\n",
            "Cosine Similarity of Q115 using BM25: 0.44634488\n",
            "Cosine Similarity of Q116 using BM25: 0.4014529\n",
            "Cosine Similarity of Q117 using BM25: 0.49096864\n",
            "Cosine Similarity of Q118 using BM25: 0.57289356\n",
            "Cosine Similarity of Q119 using BM25: 0.50445527\n",
            "Cosine Similarity of Q120 using BM25: 0.46259183\n",
            "Cosine Similarity of Q121 using BM25: 0.3907827\n",
            "Cosine Similarity of Q122 using BM25: 0.31318754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Avg of the BM25 Accuracy:\",sum(BM25Lst)/len(BM25Lst))"
      ],
      "metadata": {
        "id": "D8TZZ44bDTSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a87549cf-11a9-4aa6-8c14-e148e0110f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg of the BM25 Accuracy: 0.35737935596001824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_standard_deviation(numbers):\n",
        "    # Step 1: Calculate the mean\n",
        "    mean = sum(numbers) / len(numbers)\n",
        "\n",
        "    # Step 2: Calculate the variance\n",
        "    variance = sum((x - mean) ** 2 for x in numbers) / len(numbers)\n",
        "\n",
        "    # Step 3: Calculate the standard deviation\n",
        "    standard_deviation = math.sqrt(variance)\n",
        "\n",
        "    return standard_deviation\n",
        "\n",
        "# Example usage\n",
        "# numbers = [10, 12, 23, 23, 16, 23, 21, 16]\n",
        "# result = calculate_standard_deviation(numbers)\n",
        "# print(\"Standard Deviation:\", result)\n"
      ],
      "metadata": {
        "id": "rqWYgl5iSO4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result = calculate_standard_deviation(faissLst)\n",
        "print(\"Standard Deviation FAISS:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T1V3gzeSO05",
        "outputId": "1ef62443-02c5-4669-c94d-ed2419a57d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard Deviation FAISS: 0.09677355059923742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result = calculate_standard_deviation(DPRLst)\n",
        "print(\"Standard Deviation DPR:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0Bo_Q4ISOuu",
        "outputId": "f31b7ea6-701d-4955-f16f-c90054d90157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard Deviation DPR: 0.10851683301489061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result = calculate_standard_deviation(BM25Lst)\n",
        "print(\"Standard Deviation BM25:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Aeog_36SOrE",
        "outputId": "ae44dd11-51d9-4b37-8a2a-b55bfb105699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard Deviation BM25: 0.09636098385384173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aJnYuOUpSOn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DP-h8_w_SOk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BM25Lst = []\n",
        "\n",
        "for i in range(len(query)):\n",
        "  q = query[i]\n",
        "  ans = actual_answer[i]\n",
        "  context = bm25_retrieve_context(q)\n",
        "  context = ' '.join(context)\n",
        "  # print(len(context))\n",
        "  # print()\n",
        "  similarity_score = evaluate_answer(str(q), str(context), str(actual_answer))\n",
        "  print(f\"Cosine Similarity of Q{i} using BM25:\", similarity_score)\n",
        "  BM25Lst.append(similarity_score)\n"
      ],
      "metadata": {
        "id": "e3u9NBre5hHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Avg of the DPR Accuracy:\",sum(BM25Lst)/len(BM25Lst))"
      ],
      "metadata": {
        "id": "zQrxmqhx5hCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generation using Gemini**"
      ],
      "metadata": {
        "id": "OQmaNQRX7iBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "geminiApi = \"AIzaSyDigNdmQETuM3o9yxq1Y-ASKsg84_xMHNM\""
      ],
      "metadata": {
        "id": "art1-Ey95g9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mGw5-gxdDacb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyDigNdmQETuM3o9yxq1Y-ASKsg84_xMHNM\")\n",
        "model = genai.GenerativeModel('gemini-pro')\n"
      ],
      "metadata": {
        "id": "X-9I2QFA5g68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "K_5nOZDw5g4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"Write a story about a magic backpack.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "KBE1Ign55g1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = f\"{retrieved_context}\\n\\nAnswer the following question based on the context above :\\n{user_query}\"\n",
        "# # and you have to generate the answer even if the context is not releted\n",
        "#     # Generate the response using Gemini API\n",
        "#     response = model.generate_content(prompt)\n",
        "\n",
        "#     # Return the generated answer text\n",
        "#     return response.text"
      ],
      "metadata": {
        "id": "EVlsl5CN5gzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_BM25Lst = []\n",
        "\n",
        "for i in range(len(query)):\n",
        "  q = query[i]\n",
        "  ans = actual_answer[i]\n",
        "  context = bm25_retrieve_context(q)\n",
        "  context = ' '.join(context)\n",
        "  prompt = f\"{context}\\n\\nAnswer the following question based on the context above :\\n{str(q)}\"\n",
        "  response = model.generate_content(prompt)\n",
        "  print(response.text)\n",
        "  print()\n",
        "  # similarity_score = calculate_cosine_similarity(response, actual_answer)\n",
        "  # print(f\"Cosine Similarity of Q{i} using BM25:\", similarity_score)\n",
        "  # G_BM25Lst.append(similarity_score)\n"
      ],
      "metadata": {
        "id": "oCQfAO3Z5gvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D7Ep6MnI5gtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nIqaxlRD5gpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ygVJ1VeP5gfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GsXinRd25gdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HP7JUAze5gaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7TbFVCGd5gXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H73-N88h5gUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rMJiuqPCv0og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uZXkd-DQv0lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OlDYZQQ7o6nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zhyibC1o6k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mq7YiAcRo6ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oE_Nx3TAo6fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geminiApi = \"AIzaSyDigNdmQETuM3o9yxq1Y-ASKsg84_xMHNM\""
      ],
      "metadata": {
        "id": "wQOMd0blDWw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=\"AIzaSyDigNdmQETuM3o9yxq1Y-ASKsg84_xMHNM\")\n",
        "model = genai.GenerativeModel('gemini-pro')\n"
      ],
      "metadata": {
        "id": "C9IwzbhrY-u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure the API key\n",
        "genai.configure(api_key=\"AIzaSyDigNdmQETuM3o9yxq1Y-ASKsg84_xMHNM\")\n",
        "\n",
        "# Define the function to generate answers using Gemini API\n",
        "def generate_answer_with_gemini(retrieved_context, user_query):\n",
        "    \"\"\"\n",
        "    Generates an answer using the Gemini API based on retrieved context and user query.\n",
        "\n",
        "    Args:\n",
        "        retrieved_context (str): The context retrieved from the RAG pipeline.\n",
        "        user_query (str): The user's question or query.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer text.\n",
        "    \"\"\"\n",
        "    # Formulate the prompt\n",
        "    print(retrieved_context)\n",
        "    prompt = f\"{retrieved_context}\\n\\nAnswer the following question based on the context above and you to generate the answer even if the context is not releted:\\n{user_query}\"\n",
        "\n",
        "    # Generate the response using Gemini API\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    # Return the generated answer text\n",
        "    return response.text\n",
        "\n"
      ],
      "metadata": {
        "id": "W9bNrdHg__bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"What did Dhritarashtra say in the first chapter?\"\n",
        "b = DPR_generate_answer(a)\n",
        "# c = geminiResponse(query, b)\n",
        "# Example usage\n",
        "# retrieved_context = \"Your retrieved context from RAG goes here.\"\n",
        "# user_query = \"Your user query goes here.\"\n",
        "\n",
        "# Call the function and print the answer\n",
        "answer = generate_answer_with_gemini(b, a)\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "iNleXRKvAaXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Answer:\", c)"
      ],
      "metadata": {
        "id": "UYfUF8q-AbSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8HUANoTcvDj"
      },
      "source": [
        "Testing, Please don't edit it the following code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds858-uqcMKk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"GeetaQA.csv\")\n",
        "\n",
        "df['LLMResponse'] = df['Question'].apply(lambda query: llmResponse(query, generate_answer(query)))\n",
        "\n",
        "df.to_csv(\"LLM_file.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bxce-Cfdilvc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "df = pd.read_csv('LLM_file.csv')\n",
        "\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "similarity_scores = []\n",
        "for answer, llm_response in zip(df['Answers'], df['LLMResponse']):\n",
        "    embedding1 = model.encode(answer, convert_to_tensor=True)\n",
        "    embedding2 = model.encode(llm_response, convert_to_tensor=True)\n",
        "    score = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
        "    similarity_scores.append(score)\n",
        "\n",
        "df['SimilarityScore'] = similarity_scores\n",
        "\n",
        "df.to_csv('score_file.csv', index=False)\n",
        "\n",
        "print(\"Similarity scores added and saved to updated_file.csv.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing Part second in a good way**"
      ],
      "metadata": {
        "id": "ZIiLcY5E5Hup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary Imports\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModel, DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "import numpy as np\n",
        "import torch\n",
        "import faiss\n"
      ],
      "metadata": {
        "id": "lwrzqwBv5HXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the actual questions and answers from GeetaQA - Sheet1.csv\n",
        "qa_df = pd.read_csv('GeetaQA - Sheet1.csv')\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer for calculating similarity\n",
        "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "FiF2e14m5Ujb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DPR model and tokenizer for the question encoder\n",
        "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "question_model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "\n",
        "# Load DPR model and tokenizer for the context (passage) encoder\n",
        "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "context_model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n"
      ],
      "metadata": {
        "id": "5XyCP9Py5YTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get question embeddings\n",
        "def DPR_get_question_embedding(text):\n",
        "    inputs = question_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        embedding = question_model(**inputs).pooler_output\n",
        "    return embedding.cpu().numpy()"
      ],
      "metadata": {
        "id": "S18i4_6b5dqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get context (translation) embeddings\n",
        "def DPR_get_context_embedding(text):\n",
        "    inputs = context_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        embedding = context_model(**inputs).pooler_output\n",
        "    return embedding.cpu().numpy()"
      ],
      "metadata": {
        "id": "aS20pDnD5xGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create FAISS index for context embeddings\n",
        "context_embeddings = np.vstack([DPR_get_context_embedding(context) for context in qa_df['Answer']])\n",
        "embedding_dim = context_embeddings.shape[1]  # Dimension of the embeddings\n",
        "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "faiss_index.add(context_embeddings)"
      ],
      "metadata": {
        "id": "TLGGt2y757fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DPR and FAISS retrieval functions\n",
        "def DPR_generate_answer(query):\n",
        "    query_embedding = DPR_get_question_embedding(query)\n",
        "    distances, indices = faiss_index.search(query_embedding, k=3)\n",
        "    results = qa_df.iloc[indices[0]]['Answer'].values\n",
        "    return ' '.join(results)\n"
      ],
      "metadata": {
        "id": "NoDVOq3U6Zm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FAISS_generate_answer(query):\n",
        "    query_embedding = DPR_get_question_embedding(query)\n",
        "    distances, indices = faiss_index.search(query_embedding, k=3)\n",
        "    results = qa_df.iloc[indices[0]]['Answer'].values\n",
        "    return ' '.join(results)\n"
      ],
      "metadata": {
        "id": "doCh9gME9HmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate similarity between generated and actual answers\n",
        "def calculate_similarity(gen_answer, actual_answer, model, tokenizer):\n",
        "    inputs_gen = tokenizer(gen_answer, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    inputs_actual = tokenizer(actual_answer, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen_embedding = model(**inputs_gen).pooler_output\n",
        "        actual_embedding = model(**inputs_actual).pooler_output\n",
        "\n",
        "    # Convert embeddings to numpy arrays\n",
        "    gen_embedding_np = gen_embedding.cpu().numpy()\n",
        "    actual_embedding_np = actual_embedding.cpu().numpy()\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = cosine_similarity(gen_embedding_np, actual_embedding_np).item()\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "KbUikG9H9Ntm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compare generated answers using DPR and FAISS\n",
        "def evaluate_retrieval_methods():\n",
        "    dpr_similarities = []\n",
        "    faiss_similarities = []\n",
        "\n",
        "    for index, row in qa_df.iterrows():\n",
        "        query = row['Question']\n",
        "        actual_answer = row['Answer']\n",
        "\n",
        "        # Get context using DPR and FAISS\n",
        "        dpr_context = DPR_generate_answer(query)\n",
        "        # dpr_context = DPR_retrieve_similar_verses(query)\n",
        "        faiss_context = FAISS_generate_answer(query)\n",
        "        # faiss_context = FAISS_retrieve_similar_verses(query)\n",
        "\n",
        "        # Generate answers using llmResponse\n",
        "        dpr_gen_answer = llmResponse(query, dpr_context)\n",
        "        faiss_gen_answer = llmResponse(query, faiss_context)\n",
        "\n",
        "        # Calculate similarity between generated and actual answers\n",
        "        dpr_similarity = calculate_similarity(dpr_gen_answer, actual_answer, bert_model, bert_tokenizer)\n",
        "        faiss_similarity = calculate_similarity(faiss_gen_answer, actual_answer, bert_model, bert_tokenizer)\n",
        "\n",
        "        # Store similarities for averaging\n",
        "        dpr_similarities.append(dpr_similarity)\n",
        "        faiss_similarities.append(faiss_similarity)\n",
        "\n",
        "    # Calculate average similarity scores\n",
        "    avg_dpr_similarity = np.mean(dpr_similarities)\n",
        "    avg_faiss_similarity = np.mean(faiss_similarities)\n",
        "\n",
        "    return avg_dpr_similarity, avg_faiss_similarity"
      ],
      "metadata": {
        "id": "q1gZFhah9RIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage to compare results\n",
        "avg_dpr_similarity, avg_faiss_similarity = evaluate_retrieval_methods()\n",
        "\n",
        "print(f\"Average DPR Similarity: {avg_dpr_similarity}\")\n",
        "print(f\"Average FAISS Similarity: {avg_faiss_similarity}\")\n"
      ],
      "metadata": {
        "id": "WNwmZ7G69Vm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine which retrieval method performs better\n",
        "if avg_dpr_similarity > avg_faiss_similarity:\n",
        "    print(\"DPR retrieval method performs better on average.\")\n",
        "else:\n",
        "    print(\"FAISS retrieval method performs better on average.\")"
      ],
      "metadata": {
        "id": "OBaXXVaw9YZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Third way of testing and it realy good**\n"
      ],
      "metadata": {
        "id": "XCHIqfVYk8lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "g0GdUiz5k8TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure embedding dimensions match by checking query embedding shapes\n",
        "def ensure_embedding_dimension(embedding, target_dim):\n",
        "    if embedding.shape[-1] != target_dim:\n",
        "        embedding = np.resize(embedding, (1, target_dim))\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "G_FWzGUc9nG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the QA dataset with questions and actual answers\n",
        "qa_df = pd.read_csv('GeetaQA - Sheet1.csv')  # Ensure this path is correct\n",
        "questions = qa_df['Question'].values\n",
        "actual_answers = qa_df['Answer'].values"
      ],
      "metadata": {
        "id": "vfrb6PEwlPZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT model and tokenizer for embedding-based similarity calculation\n",
        "similarity_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "similarity_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "t55TOdfflaqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get sentence embeddings for similarity comparison\n",
        "def get_embedding(text):\n",
        "    inputs = similarity_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = similarity_model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "qOKdADRcldWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure FAISS and DPR retrieval functions return embeddings with consistent dimensions\n",
        "embedding_dimension = get_embedding(\"sample text\").shape[1]\n",
        "embedding_dimension"
      ],
      "metadata": {
        "id": "Mp8KbFf8uLWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define FAISS retrieval function with dimensionality check\n",
        "def FAISS_retrieve_similar_verses(query, top_k=5):\n",
        "    query_embedding = get_embedding(query).numpy().reshape(1, -1)\n",
        "    if query_embedding.shape[1] != index.d:\n",
        "        query_embedding = ensure_embedding_dimension(query_embedding, index.d)\n",
        "\n",
        "    # Search FAISS index for the nearest neighbors\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # Get the most similar content (without the chapter and question)\n",
        "    results = faiss_df.iloc[indices[0]]['translation'].values  # Ensure 'faiss_df' exists with column 'translation'\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "T7wDYus2uPQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity function using cosine similarity\n",
        "def calculate_similarity(answer_1, answer_2):\n",
        "    embedding_1 = get_embedding(answer_1).numpy()\n",
        "    embedding_2 = get_embedding(answer_2).numpy()\n",
        "    similarity_score = cosine_similarity(embedding_1, embedding_2)[0][0]\n",
        "    return similarity_score"
      ],
      "metadata": {
        "id": "WVLJ1djxuZ6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store similarity scores\n",
        "faiss_similarities = []\n",
        "dpr_similarities = []"
      ],
      "metadata": {
        "id": "2_L4AGQ4ucu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each question in the QA dataset\n",
        "for i, question in enumerate(questions):\n",
        "    # Retrieve FAISS and DPR contexts\n",
        "    faiss_context = ' '.join(FAISS_retrieve_similar_verses(question))\n",
        "    dpr_context = ' '.join(DPR_retrieve_similar_verses(question))\n",
        "\n",
        "    # Generate answers using the retrieved contexts\n",
        "    faiss_generated_answer = llmResponse(question, faiss_context)\n",
        "    dpr_generated_answer = llmResponse(question, dpr_context)\n",
        "    actual_answer = actual_answers[i]\n",
        "\n",
        "    # Ensure embeddings have matching dimensions before comparison\n",
        "    faiss_generated_answer_embedding = ensure_embedding_dimension(get_embedding(faiss_generated_answer).numpy(), embedding_dimension)\n",
        "    dpr_generated_answer_embedding = ensure_embedding_dimension(get_embedding(dpr_generated_answer).numpy(), embedding_dimension)\n",
        "    actual_answer_embedding = ensure_embedding_dimension(get_embedding(actual_answer).numpy(), embedding_dimension)\n",
        "\n",
        "    # Calculate similarities for both FAISS and DPR\n",
        "    faiss_similarity = cosine_similarity(faiss_generated_answer_embedding, actual_answer_embedding)[0][0]\n",
        "    dpr_similarity = cosine_similarity(dpr_generated_answer_embedding, actual_answer_embedding)[0][0]\n",
        "\n",
        "    # Append similarity scores\n",
        "    faiss_similarities.append(faiss_similarity)\n",
        "    dpr_similarities.append(dpr_similarity)\n",
        "\n",
        "    # Print individual question results\n",
        "    print(f\"Q{i+1} FAISS answer similarity: {faiss_similarity:.4f}\")\n",
        "    print(f\"Q{i+1} DPR answer similarity: {dpr_similarity:.4f}\")"
      ],
      "metadata": {
        "id": "mYRnsB3Lue_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Calculate and print the average similarities\n",
        "average_faiss_similarity = sum(faiss_similarities) / len(faiss_similarities)\n",
        "average_dpr_similarity = sum(dpr_similarities) / len(dpr_similarities)"
      ],
      "metadata": {
        "id": "0mWZdj8Huh0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nAverage answer similarity for FAISS: {average_faiss_similarity:.4f}\")\n",
        "print(f\"Average answer similarity for DPR: {average_dpr_similarity:.4f}\")"
      ],
      "metadata": {
        "id": "u-Peen9lunrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare and print which retrieval method has higher average similarity\n",
        "if average_faiss_similarity > average_dpr_similarity:\n",
        "    print(\"FAISS retrieval has a higher average answer similarity.\")\n",
        "else:\n",
        "    print(\"DPR retrieval has a higher average answer similarity.\")"
      ],
      "metadata": {
        "id": "GRVJ8B5ZurZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}